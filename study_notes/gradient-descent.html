<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Deep Dive: How Models Learn with Gradient Descent</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #f8fafc; color: #1e293b; }
        .card { background-color: white; border-radius: 1rem; padding: 2rem; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1); display: flex; flex-direction: column; }
        .math-box { background-color: #f1f5f9; border-left: 4px solid #64748b; padding: 1rem 1.5rem; border-radius: 0.5rem; font-family: 'ui-monospace', 'SFMono-Regular', Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size: 1rem; overflow-x: auto; color: #1e293b; }
        .highlight { background: linear-gradient(to right, #2563eb, #4f46e5); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-weight: 700; }
        .action-button { background-color: #2563eb; color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; font-weight: 600; transition: all 0.2s; border: none; cursor: pointer; text-align: center; }
        .action-button:hover { background-color: #1d4ed8; }
        .action-button:disabled { background-color: #93c5fd; cursor: not-allowed; }
        .text-slope { color: #7e22ce; }
        .text-intercept { color: #0f766e; }
        .text-learning-rate { color: #db2777; }
        .text-gradient { color: #ca8a04; }
    </style>
</head>
<body class="text-gray-800">

    <div class="container mx-auto p-4 md:p-8">
        
        <header class="text-center mb-12">
            <h1 class="text-4xl md:text-5xl font-bold mb-2">
                How Models Learn: <span class="highlight">The Gradient Descent Algorithm</span>
            </h1>
            <p class="text-lg text-gray-500">From abstract idea to a concrete set of steps.</p>
        </header>

        <div class="grid grid-cols-1 gap-8">
            
            <div class="card">
                <h2 class="text-3xl font-bold mb-4">The Goal: Finding the Bottom of the Valley</h2>
                <p class="mb-4 text-gray-600">We've established that our goal is to find the values for our weights (<strong class="text-intercept"><code>θ₀</code></strong> and <strong class="text-slope"><code>θ₁</code></strong>) that minimize our Cost Function (MSE). We can visualize this as an "error landscape," a 3D surface where the low points represent low error.</p>
                <p class="mb-4 text-gray-600"><strong>Analogy:</strong> Imagine this error landscape is a real, physical valley. The position of a ball in this valley is defined by its coordinates: the current <strong class="text-intercept"><code>θ₀</code></strong> and <strong class="text-slope"><code>θ₁</code></strong>. The height of the ball is the current error. **Gradient Descent** is the process of letting the ball go and allowing it to roll downhill under the force of "gravity" (the gradient) until it settles at the very bottom—the point of minimum error.</p>
            </div>

            <div class="card">
                <h2 class="text-3xl font-bold mb-4">The Update Rule: The Core of Learning</h2>
                <p class="mb-4 text-gray-600">The entire learning process is captured in a single, elegant update rule, which is applied repeatedly to each weight in the model. For any given weight <code>θ</code>, the rule is:</p>
                <div class="math-box text-center text-lg">
                    θ<sub>new</sub> = θ<sub>old</sub> - <span class="text-learning-rate">α</span> * <span class="text-gradient">∇J(θ<sub>old</sub>)</span>
                </div>
                <ul class="mt-6 space-y-4 text-sm">
                    <li><strong><code>θ<sub>new</sub></code></strong>: This is the new, slightly improved value for our weight after taking one step.</li>
                    <li><strong><code>θ<sub>old</sub></code></strong>: This is the current value of our weight.</li>
                    <li><strong><code>-</code> (The Minus Sign)</strong>: This is crucial. It tells us to go in the <em>opposite</em> direction of the gradient. Since the gradient points uphill (the direction of steepest *ascent*), we subtract it to go downhill.</li>
                    <li><strong class="text-learning-rate"><code>α</code> (alpha)</strong>: The <strong>Learning Rate</strong>. This is our "step size." It's a small number (e.g., 0.01) that controls how big of a step we take. 
                        <em>Analogy: If you're walking down a mountain in the fog, you take small, careful steps (low learning rate) to avoid falling off a cliff. If you take giant leaps (high learning rate), you might overshoot the bottom of the valley entirely.</em></li>
                    <li><strong class="text-gradient"><code>∇J(θ<sub>old</sub>)</code></strong>: The <strong>Gradient of the Cost Function</strong>. This is the heart of the calculation. It's a term from calculus that precisely measures two things:
                        <ol class="list-alpha list-inside ml-4">
                            <li>The <strong>direction</strong> of the steepest slope at our current position.</li>
                            <li>The <strong>steepness</strong> of that slope (a bigger value means we're on a steeper part of the hill).</li>
                        </ol>
                    </li>
                </ul>
            </div>

            <div class="card">
                <h2 class="text-3xl font-bold mb-4">The Gradient in Detail: The "Gravity Pull" on Our Weights</h2>
                <p class="mb-4 text-gray-600">The gradient <code>∇J</code> is made up of **partial derivatives**. We calculate one for each weight. For simple linear regression, we have two update rules that run simultaneously:</p>
                
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <p class="mb-2 font-semibold">Update Rule for Intercept (<strong class="text-intercept"><code>θ₀</code></strong>)</p>
                        <div class="math-box text-sm">θ₀ := θ₀ - <span class="text-learning-rate">α</span> * (1/n) * Σ (h(xᵢ) - yᵢ)</div>
                        <p class="mt-2 text-xs text-gray-500">This part of the gradient, <code>(1/n) * Σ (h(xᵢ) - yᵢ)</code>, is the <em>average error</em>. It tells us: "On average, is our line too high or too low?" If the average error is positive, we need to decrease <code>θ₀</code> to shift the whole line down, and vice versa.</p>
                    </div>
                    <div>
                        <p class="mb-2 font-semibold">Update Rule for Slope (<strong class="text-slope"><code>θ₁</code></strong>)</p>
                        <div class="math-box text-sm">θ₁ := θ₁ - <span class="text-learning-rate">α</span> * (1/n) * Σ (h(xᵢ) - yᵢ) * xᵢ</div>
                        <p class="mt-2 text-xs text-gray-500">This gradient is the <em>average error, weighted by the input value <code>xᵢ</code></em>. It tells us how the error relates to the input. For points with a large <code>xᵢ</code> value, the error has a much bigger impact on the slope adjustment. This makes sense: adjusting the slope pivots the line, and this pivot has the greatest effect on points far from the y-axis.</p>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2 class="text-3xl font-bold mb-4">Interactive Demo: Take One Step at a Time</h2>
                <p class="mb-6 text-gray-600">This demo lets you perform Gradient Descent manually. Start with the initial random line. Click "Take One Step" to see the algorithm perform a single update. The calculations on the right will show you the exact gradient values and how the weights are updated in real-time. Watch the line slowly "descend" towards the best fit!</p>
                <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
                    <div class="lg:col-span-2 h-[450px] relative rounded-lg bg-gray-50 p-2">
                        <canvas id="gdChart"></canvas>
                    </div>
                    <div class="flex flex-col justify-center space-y-4">
                        <div class="grid grid-cols-2 gap-2">
                            <button id="stepBtn" class="action-button">Take One Step</button>
                            <button id="resetBtn" class="action-button secondary-button">Reset</button>
                        </div>
                        <div class="card p-4 bg-gray-50">
                            <h3 class="font-bold text-lg mb-2">Live Calculations</h3>
                            <div class="text-sm space-y-2">
                                <p><strong>MSE:</strong> <span id="mseVal" class="font-mono">0.00</span></p>
                                <p><strong class="text-gradient">Gradient for θ₀:</strong> <span id="grad0Val" class="font-mono">0.00</span></p>
                                <p><strong class="text-gradient">Gradient for θ₁:</strong> <span id="grad1Val" class="font-mono">0.00</span></p>
                                <hr class="my-2">
                                <p><strong class="text-intercept">Current θ₀:</strong> <span id="theta0Val" class="font-mono">0.00</span></p>
                                <p><strong class="text-slope">Current θ₁:</strong> <span id="theta1Val" class="font-mono">0.00</span></p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2 class="text-3xl font-bold mb-4">Beyond Basic Gradient Descent: Other Optimization Methods</h2>
                <p class="mb-4 text-gray-600">The method we've just explored is technically called **Batch Gradient Descent**, because it calculates the gradient using the *entire batch* of training data for every single step. This is accurate, but can be very slow on large datasets. Data scientists use variations to speed things up.</p>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-8 mt-6">
                     <div>
                        <h3 class="text-xl font-bold mb-2">Stochastic Gradient Descent (SGD)</h3>
                        <p class="mb-4 text-sm text-gray-600"><strong>Analogy: The Impatient Ball.</strong> Instead of calculating the slope of the entire valley to decide where to roll, the SGD ball just looks at the slope of the single pebble it's touching (one data point) and immediately rolls based on that tiny piece of information. It's much faster per step, but the path it takes is noisy and erratic. It zig-zags its way towards the bottom of the valley instead of taking a smooth path.</p>
                     </div>
                      <div>
                        <h3 class="text-xl font-bold mb-2">Mini-Batch Gradient Descent</h3>
                        <p class="mb-4 text-sm text-gray-600"><strong>Analogy: The Practical Ball.</strong> This is the happy medium and the most common method used in practice. The ball doesn't look at the whole valley, nor just one pebble. Instead, it considers a *small patch of ground* around it (a "mini-batch" of, say, 32 or 64 data points), checks the average slope of that patch, and then rolls. It provides a good balance between the accuracy of Batch GD and the speed of SGD.</p>
                     </div>
                     <div>
                        <h3 class="text-xl font-bold mb-2">Advanced Optimizers (Adam, etc.)</h3>
                        <p class="mb-4 text-sm text-gray-600"><strong>Analogy: The Smart Ball with Momentum.</strong> Optimizers like Adam, RMSprop, and Adagrad are like giving our ball momentum. It not only knows which way is downhill (the gradient) but also remembers the direction it was already rolling. This helps it roll faster down long, gentle slopes and power through small bumps. These algorithms also adapt the learning rate automatically, making them converge much faster and more reliably.</p>
                     </div>
                </div>
            </div>

        </div>
    </div>
    
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const getElem = id => document.getElementById(id);

        const gdElements = {
            stepBtn: getElem('stepBtn'), resetBtn: getElem('resetBtn'),
            mseVal: getElem('mseVal'), grad0Val: getElem('grad0Val'),
            grad1Val: getElem('grad1Val'), theta0Val: getElem('theta0Val'),
            theta1Val: getElem('theta1Val'),
            gdCtx: getElem('gdChart').getContext('2d')
        };

        let gdChart;
        let dataPoints = [];
        let theta0 = 30, theta1 = -0.5;
        const learningRate = 0.00002;

        const generateData = () => {
            dataPoints = [];
            const trueSlope = 1.5;
            const trueIntercept = 75;
            for (let i = 0; i < 20; i++) {
                const x = Math.random() * 90 + 5;
                const noise = (Math.random() - 0.5) * 80;
                const y = Math.max(0, trueSlope * x + trueIntercept + noise);
                dataPoints.push({ x, y });
            }
        };

        const updateCalculations = () => {
            if (dataPoints.length === 0) return { mse: 0, grad0: 0, grad1: 0 };
            
            let totalError = 0, grad0 = 0, grad1 = 0;
            const n = dataPoints.length;

            dataPoints.forEach(p => {
                const prediction = theta0 + theta1 * p.x;
                const error = prediction - p.y;
                totalError += Math.pow(error, 2);
                grad0 += error;
                grad1 += error * p.x;
            });

            const mse = totalError / n;
            const finalGrad0 = grad0 / n;
            const finalGrad1 = grad1 / n;

            gdElements.mseVal.textContent = mse.toFixed(2);
            gdElements.grad0Val.textContent = finalGrad0.toFixed(2);
            gdElements.grad1Val.textContent = finalGrad1.toFixed(2);
            gdElements.theta0Val.textContent = theta0.toFixed(2);
            gdElements.theta1Val.textContent = theta1.toFixed(2);

            return { mse, grad0: finalGrad0, grad1: finalGrad1 };
        };

        const takeStep = () => {
            const { grad0, grad1 } = updateCalculations();
            
            theta0 = theta0 - learningRate * grad0;
            theta1 = theta1 - learningRate * grad1;
            
            updateCalculations();
            updateChart();
        };

        const updateChart = () => {
            gdChart.data.datasets[1].data = [
                { x: 0, y: theta0 },
                { x: 100, y: theta0 + theta1 * 100 }
            ];
            gdChart.update();
        };
        
        const reset = () => {
            theta0 = 30;
            theta1 = -0.5;
            generateData();
            gdChart.data.datasets[0].data = dataPoints;
            updateCalculations();
            updateChart();
        };

        const createChart = () => {
            gdChart = new Chart(gdElements.gdCtx, {
                type: 'scatter',
                data: {
                    datasets: [{
                        label: 'Data Points',
                        data: [],
                        backgroundColor: 'rgba(59, 130, 246, 0.7)',
                        pointRadius: 6
                    }, {
                        type: 'line',
                        label: 'Model',
                        data: [],
                        borderColor: '#ef4444',
                        borderWidth: 3
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { min: 0, max: 100 },
                        y: { min: 0, max: 300 }
                    },
                    plugins: { legend: { display: false } },
                    animation: { duration: 500 }
                }
            });
        };

        // --- INITIALIZATION ---
        createChart();
        reset();
        gdElements.stepBtn.addEventListener('click', takeStep);
        gdElements.resetBtn.addEventListener('click', reset);
    });
    </script>
</body>
</html>
