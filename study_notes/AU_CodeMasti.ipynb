{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class CoreLogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): The step size for gradient descent.\n",
    "            n_iterations (int): Number of times to loop over the training data.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta_0 = 0.0  # Bias term\n",
    "        self.theta_weights = [] # Weights for each feature\n",
    "        self.cost_history = [] # To track learning progress\n",
    "    \n",
    "    # --- 1. Sigmoid Function ---\n",
    "    def _sigmoid(self, z):\n",
    "        if z > 700:\n",
    "            return 1.0\n",
    "        elif z < -700:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "    # --- 2. Hypothesis and Helpers ---\n",
    "    def _compute_z(self, x_sample):\n",
    "        \"\"\"Computes z = theta_0 + (theta_weights . x_sample)\"\"\"\n",
    "        z = self.theta_0\n",
    "        for i in range(len(self.theta_weights)):\n",
    "            z += self.theta_weights[i] * x_sample[i]\n",
    "        return z\n",
    "\n",
    "    def _predict_probability(self, x_sample):\n",
    "        \"\"\"Our full hypothesis: h(x) = sigmoid(z)\"\"\"\n",
    "        z = self._compute_z(x_sample)\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    # --- 3. Cost Function ---\n",
    "    def _compute_cost(self, y_true, y_pred_probs):\n",
    "        m = len(y_true)\n",
    "        if m == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        total_cost = 0.0\n",
    "        epsilon = 1e-9\n",
    "        \n",
    "        for i in range(m):\n",
    "            h = y_pred_probs[i]\n",
    "            y = y_true[i]\n",
    "            h = max(epsilon, min(1.0 - epsilon, h)) # Clipping\n",
    "            cost_sample = -y * math.log(h) - (1 - y) * math.log(1 - h)\n",
    "            total_cost += cost_sample\n",
    "            \n",
    "        return total_cost / m\n",
    "\n",
    "    # --- 4. Gradient Descent ---\n",
    "    def _compute_gradients(self, X_data, y_true, y_pred_probs):\n",
    "        m = len(y_true)\n",
    "        n_features = len(self.theta_weights)\n",
    "        \n",
    "        grad_theta_0 = 0.0\n",
    "        grad_theta_weights = [0.0] * n_features\n",
    "        \n",
    "        for i in range(m):\n",
    "            x_sample = X_data[i]\n",
    "            h = y_pred_probs[i]\n",
    "            y = y_true[i]\n",
    "            error = h - y \n",
    "            \n",
    "            grad_theta_0 += error\n",
    "            for j in range(n_features):\n",
    "                grad_theta_weights[j] += error * x_sample[j]\n",
    "                \n",
    "        grad_theta_0 /= m\n",
    "        for j in range(n_features):\n",
    "            grad_theta_weights[j] /= m\n",
    "            \n",
    "        return grad_theta_0, grad_theta_weights\n",
    "\n",
    "    # --- 5. Main Training Function ---\n",
    "    def fit(self, X_data, y_data, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model using (Batch) Gradient Descent.\n",
    "        \n",
    "        Args:\n",
    "            X_data (list of lists): Training features. e.g., [[1, 2], [3, 4]]\n",
    "            y_data (list): Target labels. e.g., [0, 1]\n",
    "            verbose (bool): Whether to print cost updates.\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        if not X_data:\n",
    "            print(\"Error: X_data is empty.\")\n",
    "            return\n",
    "        \n",
    "        m_samples = len(y_data)\n",
    "        # Assume all samples have the same number of features as the first one\n",
    "        n_features = len(X_data[0]) \n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.theta_0 = 0.0\n",
    "        self.theta_weights = [0.0] * n_features\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # --- The Gradient Descent Loop ---\n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            # 1. Get predictions (probabilities) for ALL samples\n",
    "            y_pred_probs = []\n",
    "            for x_sample in X_data:\n",
    "                y_pred_probs.append(self._predict_probability(x_sample))\n",
    "            \n",
    "            # 2. Calculate the cost (for logging)\n",
    "            cost = self._compute_cost(y_data, y_pred_probs)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # 3. Calculate the gradients\n",
    "            grad_theta_0, grad_theta_weights = self._compute_gradients(\n",
    "                X_data, y_data, y_pred_probs\n",
    "            )\n",
    "            \n",
    "            # 4. Update the parameters\n",
    "            self.theta_0 -= self.learning_rate * grad_theta_0\n",
    "            for j in range(n_features):\n",
    "                self.theta_weights[j] -= self.learning_rate * grad_theta_weights[j]\n",
    "            \n",
    "            # Optional: Print progress\n",
    "            if verbose and i % (self.n_iterations // 10) == 0:\n",
    "                print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
    "\n",
    "    # --- 6. Prediction Functions ---\n",
    "    def predict_proba(self, X_data):\n",
    "        \"\"\"\n",
    "        Predict probabilities for new data.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        for x_sample in X_data:\n",
    "            probabilities.append(self._predict_probability(x_sample))\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X_data, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels (0 or 1) based on a threshold.\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X_data)\n",
    "        labels = []\n",
    "        for prob in probabilities:\n",
    "            if prob >= threshold:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- Testing CoreLogisticRegression ---\")\n",
    "    \n",
    "    # 1. Create a simple dataset\n",
    "    # X = \"Hours studied\"\n",
    "    # y = \"Passed\" (0 or 1)\n",
    "    # We expect X to be a list of lists (features per sample)\n",
    "    X_train = [[1.0], [1.5], [2.0], [2.5], [4.5], [5.0], [5.5], [6.0]]\n",
    "    y_train = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "    \n",
    "    # 2. Initialize and train the model\n",
    "    # A higher learning rate and more iterations are needed \n",
    "    # for this simple, un-scaled data.\n",
    "    model = CoreLogisticRegression(learning_rate=0.1, n_iterations=5000)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    # 3. Print the final parameters\n",
    "    print(f\"\\nFinal Bias (theta_0): {model.theta_0:.4f}\")\n",
    "    print(f\"Final Weights (theta_1): {model.theta_weights[0]:.4f}\")\n",
    "    \n",
    "    # 4. Make predictions\n",
    "    X_test = [[0.5], [3.0], [3.5], [7.0]]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = model.predict_proba(X_test)\n",
    "    # Predict labels\n",
    "    labels = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    for i in range(len(X_test)):\n",
    "        print(f\"Input: {X_test[i][0]} hours | \"\n",
    "              f\"Prob(Pass): {probs[i]:.4f} | \"\n",
    "              f\"Prediction: {labels[i]}\")\n",
    "\n",
    "    # Expected output:\n",
    "    # The decision boundary should be around 3.5\n",
    "    # [0.5] -> Low prob, predict 0\n",
    "    # [3.0] -> ~0.5 prob, might be 0 or 1 (near boundary)\n",
    "    # [3.5] -> ~0.5 prob, might be 0 or 1 (near boundary)\n",
    "    # [7.0] -> High prob, predict 1  \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
