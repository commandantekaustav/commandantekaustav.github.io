{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _sigmoid(z):\n",
    "    if z > 700: # \n",
    "        return 1.0\n",
    "    elif z < -700:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 / (1.0 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acea692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_z(theta_0, theta_1, x):\n",
    "    \"\"\"\n",
    "    What is Z? \n",
    "    what does this function compute?\n",
    "    \"\"\"\n",
    "\n",
    "    # Cardinality check\n",
    "    if (len(theta_0) != len(theta_1)):\n",
    "        raise ValueError(\"Mismatch in length/dimensions of weights and features\")\n",
    "\n",
    "    z = theta_0\n",
    "\n",
    "    for i in range(len(theta_1)):\n",
    "        z += theta_1[i] * x[i]\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_probability(theta_0, theta_1, x):\n",
    "    \"\"\"\n",
    "    What is out full hypothesis?\n",
    "    \"\"\"\n",
    "\n",
    "    z = _compute_z(theta_0, theta_1, x)\n",
    "\n",
    "    prob = _sigmoid(z)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664fbb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-09\n"
     ]
    }
   ],
   "source": [
    "print(100/3:1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ed92e",
   "metadata": {},
   "source": [
    "## The Cost Function (Binary Cross-Entropy)\n",
    "\n",
    "We need a way to measure how `wrong` our model is. In DL and LogReg, we use **\"Binary Cross-Entropy\"** or *log loss* function. It penalizes confident wrong answers heavily. \n",
    "\n",
    "The cost for a single sample would be;\n",
    "`Cost(h,y) = -y * log(h) - (1-y) * log(1-h)`\n",
    "\n",
    "If `y=1`, Cost = `-log(h)` (As, `h` -> 0, Cost -> INF)\n",
    "\n",
    "If `y=0`, Cost = `log(1-h)` (as `h` -> 1, Cost -> INF)\n",
    "\n",
    "The total cost `J` is the average cost over all `m` training samples. We must also add a tiny `epsilon` value to prevent `math.log(0)` which is undefined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281c4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_cost(y_true, y_pred_probs):\n",
    "    \"\"\"\n",
    "    Computes the Binary Cross-Entropy\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(y_true)\n",
    "\n",
    "    if m == 0:\n",
    "        # \"No training examples found\"\n",
    "        return 0.0\n",
    "\n",
    "    total_cost = 0.0\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    for i in range(m):\n",
    "        h = y_pred_probs[i]\n",
    "        y = y_true[i]\n",
    "\n",
    "        # Clip predictions to avoid log(0) error\n",
    "        h = max(epsilon, min(1.0 - epsilon, h))\n",
    "\n",
    "        cost_sample = -y * math.log(h) - (1 - y) * math.log(1 - h)\n",
    "        total_cost += cost_sample\n",
    "\n",
    "    avg_cost = total_cost / m\n",
    "\n",
    "    return avg_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ffde8",
   "metadata": {},
   "source": [
    "## `Gradient Descent` as in; The **`Learning`**\n",
    "\n",
    "This is how the model learns. We want to minimize the cost `J`. We do this by taking the partial derivative of `J` with respect to each parameter (`theta_0` and each `theta_j`) and nudging the parameters in the opposite direction.\n",
    "\n",
    "The derivatives are: (don't complain. these are easy AF.)\n",
    "\n",
    "$dJ/d{\\theta_0} = (1/m) * Σ (h(x_i) - y_i)$\n",
    "\n",
    "\n",
    "$dJ/d(\\theta_j) = (1/m) * Σ (h(x_i) - y_i) * x_{ij}  : (for j=1..n)$\n",
    "\n",
    "We'll compute these \"gradients\" and then update our parameters:\n",
    "\n",
    "$\\theta_0 = \\theta_0 - α * (dJ/d(\\theta_0))$\n",
    "\n",
    "$\\theta_j = \\theta_j - α * (dJ/d(\\theta_j))$\n",
    "\n",
    "Where `α` (alpha) is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0c4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_gradients(X_data, y_true, y_pred_probs, n_features):\n",
    "    \"\"\"\n",
    "    Computes the gradients of the cost function w.r.t. parameters.\n",
    "    \"\"\"\n",
    "    m = len(y_true)\n",
    "    \n",
    "    # Initialize gradients to zero\n",
    "    grad_theta_0 = 0.0\n",
    "    grad_theta_weights = [0.0] * n_features\n",
    "    \n",
    "    for i in range(m):\n",
    "        x_sample = X_data[i]\n",
    "        h = y_pred_probs[i]\n",
    "        y = y_true[i]\n",
    "        \n",
    "        # The error term is common to all gradients\n",
    "        error = h - y \n",
    "        \n",
    "        # Accumulate gradient for bias\n",
    "        grad_theta_0 += error\n",
    "        \n",
    "        # Accumulate gradients for weights\n",
    "        for j in range(n_features):\n",
    "            grad_theta_weights[j] += error * x_sample[j]\n",
    "            \n",
    "    # Average the gradients\n",
    "    grad_theta_0 /= m\n",
    "    for j in range(n_features):\n",
    "        grad_theta_weights[j] /= m\n",
    "        \n",
    "    return grad_theta_0, grad_theta_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182347d8",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be77ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a0fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class CoreLogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): The step size for gradient descent.\n",
    "            n_iterations (int): Number of times to loop over the training data.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta_0 = 0.0  # Bias term\n",
    "        self.theta_weights = [] # Weights for each feature\n",
    "        self.cost_history = [] # To track learning progress\n",
    "    \n",
    "    # --- 1. Sigmoid Function ---\n",
    "    def _sigmoid(self, z):\n",
    "        if z > 700:\n",
    "            return 1.0\n",
    "        elif z < -700:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "    # --- 2. Hypothesis and Helpers ---\n",
    "    def _compute_z(self, x_sample):\n",
    "        \"\"\"Computes z = theta_0 + (theta_weights . x_sample)\"\"\"\n",
    "        z = self.theta_0\n",
    "        for i in range(len(self.theta_weights)):\n",
    "            z += self.theta_weights[i] * x_sample[i]\n",
    "        return z\n",
    "\n",
    "    def _predict_probability(self, x_sample):\n",
    "        \"\"\"Our full hypothesis: h(x) = sigmoid(z)\"\"\"\n",
    "        z = self._compute_z(x_sample)\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    # --- 3. Cost Function ---\n",
    "    def _compute_cost(self, y_true, y_pred_probs):\n",
    "        m = len(y_true)\n",
    "        if m == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        total_cost = 0.0\n",
    "        epsilon = 1e-9\n",
    "        \n",
    "        for i in range(m):\n",
    "            h = y_pred_probs[i]\n",
    "            y = y_true[i]\n",
    "            h = max(epsilon, min(1.0 - epsilon, h)) # Clipping\n",
    "            cost_sample = -y * math.log(h) - (1 - y) * math.log(1 - h)\n",
    "            total_cost += cost_sample\n",
    "            \n",
    "        return total_cost / m\n",
    "\n",
    "    # --- 4. Gradient Descent ---\n",
    "    def _compute_gradients(self, X_data, y_true, y_pred_probs):\n",
    "        m = len(y_true)\n",
    "        n_features = len(self.theta_weights)\n",
    "        \n",
    "        grad_theta_0 = 0.0\n",
    "        grad_theta_weights = [0.0] * n_features\n",
    "        \n",
    "        for i in range(m):\n",
    "            x_sample = X_data[i]\n",
    "            h = y_pred_probs[i]\n",
    "            y = y_true[i]\n",
    "            error = h - y \n",
    "            \n",
    "            grad_theta_0 += error\n",
    "            for j in range(n_features):\n",
    "                grad_theta_weights[j] += error * x_sample[j]\n",
    "                \n",
    "        grad_theta_0 /= m\n",
    "        for j in range(n_features):\n",
    "            grad_theta_weights[j] /= m\n",
    "            \n",
    "        return grad_theta_0, grad_theta_weights\n",
    "\n",
    "    # --- 5. Main Training Function ---\n",
    "    def fit(self, X_data, y_data, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model using (Batch) Gradient Descent.\n",
    "        \n",
    "        Args:\n",
    "            X_data (list of lists): Training features. e.g., [[1, 2], [3, 4]]\n",
    "            y_data (list): Target labels. e.g., [0, 1]\n",
    "            verbose (bool): Whether to print cost updates.\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        if not X_data:\n",
    "            print(\"Error: X_data is empty.\")\n",
    "            return\n",
    "        \n",
    "        m_samples = len(y_data)\n",
    "        # Assume all samples have the same number of features as the first one\n",
    "        n_features = len(X_data[0]) \n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.theta_0 = 0.0\n",
    "        self.theta_weights = [0.0] * n_features\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # --- The Gradient Descent Loop ---\n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            # 1. Get predictions (probabilities) for ALL samples\n",
    "            y_pred_probs = []\n",
    "            for x_sample in X_data:\n",
    "                y_pred_probs.append(self._predict_probability(x_sample))\n",
    "            \n",
    "            # 2. Calculate the cost (for logging)\n",
    "            cost = self._compute_cost(y_data, y_pred_probs)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # 3. Calculate the gradients\n",
    "            grad_theta_0, grad_theta_weights = self._compute_gradients(\n",
    "                X_data, y_data, y_pred_probs\n",
    "            )\n",
    "            \n",
    "            # 4. Update the parameters\n",
    "            self.theta_0 -= self.learning_rate * grad_theta_0\n",
    "            for j in range(n_features):\n",
    "                self.theta_weights[j] -= self.learning_rate * grad_theta_weights[j]\n",
    "            \n",
    "            # Optional: Print progress\n",
    "            if verbose and i % (self.n_iterations // 10) == 0:\n",
    "                print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
    "\n",
    "    # --- 6. Prediction Functions ---\n",
    "    def predict_proba(self, X_data):\n",
    "        \"\"\"\n",
    "        Predict probabilities for new data.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        for x_sample in X_data:\n",
    "            probabilities.append(self._predict_probability(x_sample))\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X_data, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels (0 or 1) based on a threshold.\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X_data)\n",
    "        labels = []\n",
    "        for prob in probabilities:\n",
    "            if prob >= threshold:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e48dce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing CoreLogisticRegression ---\n",
      "Starting training...\n",
      "Iteration 0: Cost = 0.6931\n",
      "Iteration 500: Cost = 0.1241\n",
      "Iteration 1000: Cost = 0.0713\n",
      "Iteration 1500: Cost = 0.0510\n",
      "Iteration 2000: Cost = 0.0401\n",
      "Iteration 2500: Cost = 0.0332\n",
      "Iteration 3000: Cost = 0.0284\n",
      "Iteration 3500: Cost = 0.0249\n",
      "Iteration 4000: Cost = 0.0222\n",
      "Iteration 4500: Cost = 0.0200\n",
      "Training complete.\n",
      "\n",
      "Final Bias (theta_0): -9.8485\n",
      "Final Weights (theta_1): 2.9078\n",
      "\n",
      "--- Test Results ---\n",
      "Input: 0.5 hours | Prob(Pass): 0.0002 | Prediction: 0\n",
      "Input: 3.0 hours | Prob(Pass): 0.2450 | Prediction: 0\n",
      "Input: 3.5 hours | Prob(Pass): 0.5814 | Prediction: 1\n",
      "Input: 7.0 hours | Prob(Pass): 1.0000 | Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- Testing CoreLogisticRegression ---\")\n",
    "    \n",
    "    # 1. Create a simple dataset\n",
    "    # X = \"Hours studied\"\n",
    "    # y = \"Passed\" (0 or 1)\n",
    "    # We expect X to be a list of lists (features per sample)\n",
    "    X_train = [[1.0], [1.5], [2.0], [2.5], [4.5], [5.0], [5.5], [6.0]]\n",
    "    y_train = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "    \n",
    "    # 2. Initialize and train the model\n",
    "    # A higher learning rate and more iterations are needed \n",
    "    # for this simple, un-scaled data.\n",
    "    model = CoreLogisticRegression(learning_rate=0.1, n_iterations=5000)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    # 3. Print the final parameters\n",
    "    print(f\"\\nFinal Bias (theta_0): {model.theta_0:.4f}\")\n",
    "    print(f\"Final Weights (theta_1): {model.theta_weights[0]:.4f}\")\n",
    "    \n",
    "    # 4. Make predictions\n",
    "    X_test = [[0.5], [3.0], [3.5], [7.0]]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = model.predict_proba(X_test)\n",
    "    # Predict labels\n",
    "    labels = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    for i in range(len(X_test)):\n",
    "        print(f\"Input: {X_test[i][0]} hours | \"\n",
    "              f\"Prob(Pass): {probs[i]:.4f} | \"\n",
    "              f\"Prediction: {labels[i]}\")\n",
    "\n",
    "    # Expected output:\n",
    "    # The decision boundary should be around 3.5\n",
    "    # [0.5] -> Low prob, predict 0\n",
    "    # [3.0] -> ~0.5 prob, might be 0 or 1 (near boundary)\n",
    "    # [3.5] -> ~0.5 prob, might be 0 or 1 (near boundary)\n",
    "    # [7.0] -> High prob, predict 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c350e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
